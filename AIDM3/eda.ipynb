{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Advances in Data Mining**\n",
    "\n",
    "Stephan van der Putten | (s1528459) | stvdputtenjur@gmail.com  \n",
    "Theo Baart | s2370328 | s2370328@student.leidenuniv.nl\n",
    "\n",
    "### **Assignment 3**\n",
    "This assignment is concerned with performing an analysis of and execute PageRank on the wikipedia links given in the `wikilink_graph.2004-03-01.csv` file. In order to do this the assignment is split up into four subtasks with each subtask receiving its dedicated `.ipynb` file. See each specific file for details on what this notebook accomplishes.\n",
    "\n",
    "Note all implementations are based on the assignment guidelines and helper files given as well as the documentation of the used functions. Additionally, the following sources are referenced:\n",
    "  1. Mining of Masive Datasets by Jure Leskovec, Anand Rajaraman and Jeff ullman\n",
    "\n",
    "#### **Exploratory Data Analysis**\n",
    "This notebook performs an exploratory analysis on the dataset. This includes some anlaysis on the nodes and edges as well as estimating system requirements for being able to execute the PageRank algorithm.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Helper Functions**\n",
    "This section contains functions which aid and simplify the code.\n",
    "___\n",
    "The following snippet handles all imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `compute_average` function computes the average value of a one-dimensional sparse matrix.\n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `data` - the one-dimensional sparse matrix we want the average of\n",
    "  * `ignore_zero` - whether to ignore zeros [default = `True`]\n",
    "\n",
    "Additionally, it returns the following value:\n",
    "  * `average` - the average value of the given matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average(data,ignore_zero=True):\n",
    "    if ignore_zero:\n",
    "        # built in functions don't ignore zeros\n",
    "        d_sum = data.sum()\n",
    "        d_len = data.count_nonzero()\n",
    "        average = d_sum / d_len\n",
    "    else:\n",
    "        average = data.mean()\n",
    "    return average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `compute_average_link_count` function parses the nodes and computes the average number of links per node.\n",
    "\n",
    "In order to do this the function uses the following parameter:\n",
    "  * `data` - the prepped data\n",
    "  \n",
    "Additionally, it returns the following value:\n",
    "  * `average` - the average number of links per node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_link_count(data):\n",
    "    num_nodes = data.get_shape()[0]\n",
    "    num_links = data.getnnz()\n",
    "    average = num_nodes / num_links\n",
    "    return average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `compute_dead_ends_set` function parses the nodes and computes a set of all nodes which are dead_ends. Analyzing the data it is evident that for a node to be in the dataset it must either have an outgoing edge or an incoming edge. By definition a dead end has no outgoing edges and therefore it cannot be in the list of outgoing edges. Thus, the difference between the set of all edges and the set of outgoing edges is the set of dead ends.\n",
    "\n",
    "In order to do this the function uses the following parameter:\n",
    "  * `data` - the prepped data\n",
    "  \n",
    "Additionally, it returns the following value:\n",
    "  * `dead_ends` - a list of all the dead ends [in consecutive numbering]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dead_ends_set(data):\n",
    "    all_edges = set(range(data.get_shape()[1])) # get all column ID's \n",
    "    outgoing_edges = set(data.nonzero()[1]) # column ID's of outgoing edges\n",
    "#     print(all_edges)\n",
    "#     print(outgoing_edges)\n",
    "    dead_ends = all_edges - outgoing_edges\n",
    "#     print(dead_ends)\n",
    "    dead_ends = set(dead_ends)\n",
    "    return dead_ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `compute_ram_custom_format` function analyzes the ram requirements using the custom format specified on slide 17 of the instructional slideset. The RAM computation is based on the description given in Section 5.2.1 of [1].\n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `num_links` - how many links there are in the dataset\n",
    "  * `byte_format` - in what unit RAM should be returned [supports: `[GB, MB, KB, B, compute]`, default = `compute`]\n",
    "  * `bits` - how many bits are needed to store a value [default = `32` as we only need to store the row numbers and there are less than 4 million rows]\n",
    "  * `avg_links` - how many destinations there are on average per node [default = `10` based on slide 16 in the instructional slideset]\n",
    "  \n",
    "Additionally, it returns the following value:\n",
    "  * `total_bytes` - the number of bytes needed in RAM\n",
    "  * `unit_bytes` - the byte format used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ram_custom_format(num_links, byte_format = 'compute', bits = 32, avg_links = 10):\n",
    "    # transition_matrix contains num_links elements with avg_links + 1 (degree) values\n",
    "    bits_transition_matrix = (num_links * (avg_links + 1)) * bits\n",
    "    # initial vector contains num_links values.\n",
    "    bits_initial_vector = (num_links) * bits\n",
    "    total_bits = bits_transition_matrix + bits_initial_vector\n",
    "    # convert to Xbytes\n",
    "    total_bytes = total_bits / 8\n",
    "    if byte_format == 'compute':\n",
    "        total_bytes, unit_bytes = compute_x_bytes(total_bytes)\n",
    "    else:\n",
    "        total_bytes, unit_bytes = compute_x_bytes(total_bytes,byte_format)        \n",
    "    return total_bytes, unit_bytes   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `compute_ram_sparse` function analyzes the ram requirements using a sparce matrix as the implementation of the transition matrix and the initial vector. The RAM computation is based on the description given in Section 5.2.1 of [1].\n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `num_links` - how many links there are in the dataset\n",
    "  * `byte_format` - in what unit RAM should be returned [supports: `[GB, MB, KB, B, compute]`, default = `compute`]\n",
    "  * `bits` - how many bits are needed to store a value [default = `64`]\n",
    "  * `avg_links` - how many destinations there are on average per node [default = `10` based on slide 16 in the instructional slideset]\n",
    "  \n",
    "Additionally, it returns the following value:\n",
    "  * `total_bytes` - the number of bytes needed in RAM\n",
    "  * `unit_bytes` - the byte format used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ram_sparse(num_links, byte_format = 'compute', bits = 64, avg_links = 10):\n",
    "    # transition_matrix contains num_links elements with avg_links values. Additionally, values and coordinates must be stored.\n",
    "    value_bits = (num_links * avg_links) * bits\n",
    "    coordinate_bits = (num_links * avg_links) * 64 # coordinates are 32bits, 2 coordinates per value\n",
    "    bits_transition_matrix = value_bits + coordinate_bits\n",
    "    # initial vector contains num_links values.\n",
    "    bits_initial_vector = (num_links) * bits\n",
    "    total_bits = bits_transition_matrix + bits_initial_vector\n",
    "    # convert to Xbytes\n",
    "    total_bytes = total_bits / 8\n",
    "    if byte_format == 'compute':\n",
    "        total_bytes, unit_bytes = compute_x_bytes(total_bytes)\n",
    "    else:\n",
    "        total_bytes, unit_bytes = compute_x_bytes(total_bytes,byte_format)        \n",
    "    return total_bytes, unit_bytes     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `compute_ram_traditional` function analyzes the ram requirements using the traditional implementation of the transition matrix and the initial vector. Note that double precision is assumed (e.g. 64 bits per value)\n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `num_links` - how many links there are in the dataset\n",
    "  * `byte_format` - in what unit RAM should be returned [supports: `[GB, MB, KB, B, compute]`, default = `compute`]\n",
    "  * `bits` - how many bits are needed to store a value [default = `64`]\n",
    "  \n",
    "Additionally, it returns the following value:\n",
    "  * `total_bytes` - the number of bytes needed in RAM\n",
    "  * `unit_bytes` - the byte format used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ram_traditional(num_links, byte_format = 'compute', bits = 64):\n",
    "    # transition_matrix contains num_links * num_links values\n",
    "    bits_transition_matrix = (num_links * num_links) * bits\n",
    "    # initial vector contains num_links values.\n",
    "    bits_initial_vector = (num_links) * bits\n",
    "    total_bits = bits_transition_matrix + bits_initial_vector\n",
    "    # convert to Xbytes\n",
    "    total_bytes = total_bits / 8\n",
    "    if byte_format == 'compute':\n",
    "        total_bytes, unit_bytes = compute_x_bytes(total_bytes)\n",
    "    else:\n",
    "        total_bytes, unit_bytes = compute_x_bytes(total_bytes,byte_format)        \n",
    "    return total_bytes, unit_bytes   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `compute_x_bytes` function receives a value and converts it to the closest logical unit. This is defined by being the largest unit such that there is always a value before the decimal point. The largest allowed unit is `GB` and additionally the value is always rounded to 3 decimal places.\n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `total_bytes` - the value to convert\n",
    "  * `format_override` - use to manually force a format [supports: `[GB, MB, KB, B, None]`, default = `None`]\n",
    "  \n",
    "Additionally, it returns the following value:\n",
    "  * `converted_bytes` - the converted value rounded to 3 decimal places\n",
    "  * `unit_bytes` - a textual representation of the unit used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_x_bytes(total_bytes, format_override=None):\n",
    "    if format_override is not None:\n",
    "        unit_bytes = format_override\n",
    "        if format_override == 'B':\n",
    "            converted_bytes = total_bytes\n",
    "        elif format_override == 'KB':\n",
    "            converted_bytes = total_bytes / 1e3\n",
    "        elif format_override == 'MB':\n",
    "            converted_bytes = total_bytes / 1e6\n",
    "        elif format_override == 'GB':\n",
    "            converted_bytes = total_bytes / 1e9\n",
    "    else :\n",
    "        if total_bytes % 1e9 < total_bytes:\n",
    "            converted_bytes = total_bytes / 1e9\n",
    "            unit_bytes = 'GB'\n",
    "        elif total_bytes % 1e6 < total_bytes:\n",
    "            converted_bytes = total_bytes / 1e6\n",
    "            unit_bytes = 'MB'\n",
    "        elif total_bytes % 1e3 < total_bytes:\n",
    "            converted_bytes = total_bytes / 1e3\n",
    "            unit_bytes = 'KB'\n",
    "        else:\n",
    "            converted_bytes = total_bytes\n",
    "            unit_bytes = 'B'\n",
    "    converted_bytes = round(converted_bytes,3)\n",
    "    return converted_bytes, unit_bytes   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `compute_x_degrees` function parses the nodes and computes for each node the x-degree of the node.\n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `data` - the prepped data\n",
    "  * `x` - whether to compute the in- or out-degree [default = `in`]\n",
    "  \n",
    "Additionally, it returns the following value:\n",
    "  * `x_degrees` - a list of all the nodes and their x-degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_x_degrees(data, x = 'in'):\n",
    "    if x == 'in':\n",
    "        edges = data.nonzero()[0]\n",
    "        edges_length = data.get_shape()[0]\n",
    "    else:\n",
    "        edges = data.nonzero()[1]\n",
    "        edges_length = data.get_shape()[1]\n",
    "    edges_index, edges_count = np.unique(edges, return_counts=True)\n",
    "    edges_nonzero_length = len(edges_index)\n",
    "    edges_zeros = np.zeros(edges_nonzero_length)\n",
    "#     print(edges_count)\n",
    "#     print(np.log(edges_count))\n",
    "#     print(edges_index,edges_count,edges_zeros,edges_nonzero_length,edges_length)\n",
    "    x_degrees = csr_matrix((edges_count, (edges_zeros,edges_index)), shape=(1,edges_length))\n",
    "#     print(x_degrees)\n",
    "    return x_degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `plot_distribution` function plots the distribution of a one-dimensional sparse matrix.\n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `data` - the one-dimensional sparse matrix to be plotted\n",
    "  * `title` - the title to give the plot [default = `'distribution'`]\n",
    "  * `ignore_zeros` - whether to include zeros in the distrubtion [default = `True`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(data,title='Count of each Degree ',ignore_zeros=True):\n",
    "    #TODO Implement Log optionality\n",
    "     \n",
    "    data = data.toarray()[0]\n",
    "    if ignore_zeros:\n",
    "        data = data[data!=0]\n",
    "    degree, count = np.unique(data, return_counts=True)\n",
    "\n",
    "#     print(np.unique(data, return_counts=True))\n",
    "    #plotting\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(degree, count)\n",
    "    plt.title(title)\n",
    "    ax.set_xlabel('log degree')\n",
    "    ax.set_ylabel('log count')\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "#     plt.savefig('graph_in.png', dpi=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ExploratoryDataAnalysis Class**\n",
    "\n",
    "This section contains the class and functions responsible for the exploratory data analysis. Due to the limitations of `.ipynb` files the various functions will be described first and then the implementation will be shown.\n",
    "\n",
    "___\n",
    "The `__init__` function initializes the class.\n",
    "\n",
    "In order to do this the function uses the following (optional) parameter:\n",
    "  * `prepped_dataset` - the prepared link data [default: None]\n",
    "___\n",
    "The `load_prepped_data` function is responsible for retrieving the data prepped by `prep.ipynb` and loading it for exploratory data analysis.\n",
    "\n",
    "In order to do this the function uses the following parameter:\n",
    "  * `filename` - the name of the file containing the prepped data [default = `prep_data.npz`]\n",
    "___\n",
    "The `analyse_dead_ends` function analyzes the matrix and prints some data on the dead ends in the graph. A dead end refers to nodes which do not have any outgoing edges.\n",
    "\n",
    "In order to do this the function uses the following parameter:\n",
    "  * `print_set`  - whether to print the set of dead ends [default = `False`]\n",
    "___\n",
    "The `analyze_in_degrees` function analyzes the matrix and prints some data on the in-degrees of the graph. An in-degree refers to the number of incoming edges that a node has. This includes a distribution of the in-degrees as well as the average number of in-degrees.\n",
    "\n",
    "In order to do this the function uses the following parameter:\n",
    "  * `ignore_zeros_plot` - whether to plot zeros [default = `True`]\n",
    "___\n",
    "The `analyze_out_degrees` function analyzes the matrix and prints some data on the out-degrees of the graph. An out-degree refers to the number of outgoinng edges that a node has. This includes a distribution of the out-degrees as well as the average number of out-degrees.\n",
    "\n",
    "In order to do this the function uses the following parameter:\n",
    "  * `ignore_zeros_plot` - whether to plot zeros [default = `True`]\n",
    "___\n",
    "The `analyze_ram_requirements` function analyzes the transition matrix and various methods of implementing it in order to estimate the ram requirements needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExploratoryDataAnalysis():\n",
    "    def __init__(self,prepped_dataset=None):\n",
    "        self.prepped_dataset = prepped_dataset\n",
    "    \n",
    "    def load_prepped_data(self,filename = 'prep_data.npz'):\n",
    "        self.prepped_dataset = scipy.sparse.load_npz(filename)\n",
    "    \n",
    "    def analyze_dead_ends(self,print_set=False):\n",
    "        dead_ends = compute_dead_ends_set(self.prepped_dataset)\n",
    "        count_dead_ends = len(dead_ends)\n",
    "        if count_dead_ends == 0:\n",
    "            print(\"There are no dead ends\")\n",
    "            return\n",
    "        # TODO convert consecutive numbering to original number\n",
    "        if count_dead_ends == 1:\n",
    "            print(\"There is 1 dead end.\")\n",
    "        else:\n",
    "            print(\"There are \"+str(len(dead_ends))+\" dead ends.\")\n",
    "        if print_set:\n",
    "            print(\"The following set contains all nodes classified as dead ends [in consecutive numbering].\")\n",
    "            print(dead_ends)\n",
    "            \n",
    "    def analyze_in_degrees(self,ignore_zeros_plot = True):\n",
    "        in_degrees = compute_x_degrees(self.prepped_dataset, 'in')\n",
    "\n",
    "        # Distribution\n",
    "        plot_distribution(in_degrees,title='Log distribution of in-degrees',ignore_zeros=ignore_zeros_plot)\n",
    "\n",
    "        # Averages\n",
    "        average_zero = compute_average(in_degrees,ignore_zero=False)\n",
    "        average_nonzero = compute_average(in_degrees,ignore_zero=True)\n",
    "        print(\"The average in-degree (including zeros): \" + str(average_zero))\n",
    "        print(\"The average in-degree (excluding zeros): \" + str(average_nonzero))\n",
    "        \n",
    "    def analyze_out_degrees(self,ignore_zeros_plot = True):\n",
    "        out_degrees = compute_x_degrees(self.prepped_dataset,'out')\n",
    "\n",
    "        # Distribution\n",
    "        plot_distribution(out_degrees,title='Log distribution of out-degrees',ignore_zeros=ignore_zeros_plot)\n",
    "\n",
    "        # Averages\n",
    "        average_zero = compute_average(out_degrees,ignore_zero=False)\n",
    "        average_nonzero = compute_average(out_degrees,ignore_zero=True)\n",
    "        print(\"The average out-degree (including zeros): \" + str(average_zero))\n",
    "        print(\"The average out-degree (excluding zeros): \" + str(average_nonzero))\n",
    "        \n",
    "    def analyze_ram_requirements(self):\n",
    "        num_links = self.prepped_dataset.get_shape()[0]\n",
    "        average_links = compute_average_link_count(self.prepped_dataset)\n",
    "\n",
    "        ram_traditional, ram_traditional_unit = compute_ram_traditional(num_links)\n",
    "        ram_sparse, ram_sparse_unit = compute_ram_sparse(num_links,avg_links=average_links)\n",
    "        ram_custom, ram_custom_unit = compute_ram_custom_format(num_links, avg_links=average_links)\n",
    "\n",
    "        print('Implementing the transition matrix M in the traditional manner...')\n",
    "        print(f'...and storing both M and the initial vector v in RAM requires {ram_traditional} {ram_traditional_unit} of RAM')\n",
    "        print('------------------')\n",
    "        print('Implementing the transition matrix M using a sparse matrix...')\n",
    "        print(f'...and storing both M and the initial vector v in RAM requires {ram_sparse} {ram_sparse_unit} of RAM')\n",
    "        print('------------------')\n",
    "        print('Implementing the transition matrix M using the format specified on slide 17 of the instructions...')\n",
    "        print(f'...and storing both M and the initial vector v in RAM requires {ram_custom} {ram_custom_unit} of RAM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Program Execution**\n",
    "This section is concerned with parsing the input arguments and determining the execution flow of the program.\n",
    "___\n",
    "The `main` function handles the command line arguments and is responsible for the main flow of the program.\n",
    "\n",
    "In order to do this the function uses the following parameter:\n",
    "  * `path` - the location for the link data file [default = `prep_data.npz`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main(path = 'prep_data.npz'):\n",
    "    eda = ExploratoryDataAnalysis()\n",
    "    eda.load_prepped_data(path)\n",
    "    \n",
    "    eda.analyze_dead_ends()\n",
    "    print('\\n')\n",
    "    eda.analyze_in_degrees()\n",
    "    print('\\n')\n",
    "    eda.analyze_out_degrees()\n",
    "    print('\\n')\n",
    "    eda.analyze_ram_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet passes the start of the program and the command line arguments to the `main` function.\n",
    "\n",
    "The following command line argument is expected:\n",
    "  * `path` - the location of the `prep_data.npz` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b979dcfcf3e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    filepath = sys.argv[1]\n",
    "    main(path=filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet triggers the manual execuation of the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
