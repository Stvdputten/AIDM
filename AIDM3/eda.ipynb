{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Advances in Data Mining**\n",
    "\n",
    "Stephan van der Putten | (s1528459) | stvdputtenjur@gmail.com  \n",
    "Theo Baart | s2370328 | s2370328@student.leidenuniv.nl\n",
    "\n",
    "### **Assignment 3**\n",
    "This assignment is concerned with performing an analysis of and execute PageRank on the wikipedia links given in the `wikilink_graph.2004-03-01.csv` file. In order to do this the assignment is split up into four subtasks with each subtask receiving its dedicated `.ipynb` file. See each specific file for details on what this notebook accomplishes.\n",
    "\n",
    "Note all implementations are based on the assignment guidelines and helper files given as well as the documentation of the used functions. \n",
    "\n",
    "#### **Exploratory Data Analysis**\n",
    "This notebook performs an exploratory analysis on the dataset. This includes some anlaysis on the nodes and edges as well as estimating system requirements for being able to execute the PageRank algorithm.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet handles all imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_prepped_data` function is responsible for retrieving the data prepped by `prep.ipynb` and loading it for exploratory data analysis.\n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `filename` - the name of the file containing the prepped data [default = `prepped-data.npy`]\n",
    "  \n",
    "Additionally, it returns the following values:\n",
    "  * `data` - an array representing the prepped data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prepped_data(filename = 'prepped_data.npz'):\n",
    "    data = scipy.sparse.load_npz(filename)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet triggers data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_prepped_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dead Ends**\n",
    "This section is concerned with determining how many dead ends there are. A dead end refers to nodes which do not have any outgoing edges.\n",
    "___\n",
    "The `compute_dead_ends_set` function parses the nodes and computes a set of all nodes which are dead_ends. Analyzing the data it is evident that for a node to be in the dataset it must either have an outgoing edge or an incoming edge. By definition a dead end has no outgoing edges and therefore it cannot be in the list of outgoing edges. Thus, the difference between the set of all edges and the set of outgoing edges is the set of dead ends.\n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `data` - the prepped data\n",
    "  \n",
    "Additionally, it returns the following values:\n",
    "  * `dead_ends` - a list of all the dead ends [in consecutive numbering]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dead_ends_set(data):\n",
    "    all_edges = set(range(data.get_shape()[1])) # get all column ID's \n",
    "    outgoing_edges = set(data.nonzero()[1]) # column ID's of outgoing edges\n",
    "    print(all_edges)\n",
    "    print(outgoing_edges)\n",
    "    dead_ends = all_edges - outgoing_edges\n",
    "    print(dead_ends)\n",
    "    dead_ends = set(dead_ends)\n",
    "    return dead_ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `analyse_dead_ends` function analyzes the matrix and prints some data on the dead_ends in the graph.\n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `data` - the prepped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dead_ends(data):\n",
    "    dead_ends = compute_dead_ends_set(data)\n",
    "    count_dead_ends = len(dead_ends)\n",
    "    if count_dead_ends == 0:\n",
    "        print(\"There are no dead ends\")\n",
    "        return\n",
    "    # TODO convert consecutive numbering to original number\n",
    "    if count_dead_ends == 1:\n",
    "        print(\"There is 1 dead end.\")\n",
    "        print(\"The following node is classified as a dead end [in consecutive numbering].\")\n",
    "    else:\n",
    "        print(\"There are \"+str(len(dead_ends))+\" dead ends.\")\n",
    "        print(\"The following set contains all nodes classified as dead ends [in consecutive numbering].\")\n",
    "    print(dead_ends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet triggers the dead end analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dead_ends(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 2]\n",
      " [0 3]\n",
      " [2 1]\n",
      " [2 3]\n",
      " [3 1]\n",
      " [3 0]]\n",
      "{0, 1, 2, 3}\n",
      "{0, 1, 2, 3}\n",
      "set()\n",
      "There are no dead ends\n"
     ]
    }
   ],
   "source": [
    "# TODO REMOVE ME I AM TEMPORARY!\n",
    "no_dead_ends = [[0,2],[1,2],[0,3],[2,1],[2,3],[3,1],[3,0]]\n",
    "one_dead_end = [[0,2],[0,3],[2,1],[2,3],[3,1],[3,0]]\n",
    "two_dead_ends = [[0,2],[0,3],[2,1],[2,3]]\n",
    "temp_data = no_dead_ends\n",
    "# temp_data = one_dead_end\n",
    "# temp_data = two_dead_ends\n",
    "temp_data = np.array(temp_data)\n",
    "c = temp_data[:,0]\n",
    "r = temp_data[:,1]\n",
    "d = np.ones(len(c))\n",
    "max_c = 4\n",
    "print(temp_data)\n",
    "temp_data = csr_matrix((d,(r,c)), shape=(max_c, max_c), dtype=np.uint16)\n",
    "scipy.sparse.save_npz('prepped_data',temp_data)\n",
    "data = load_prepped_data()\n",
    "analyze_dead_ends(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 1],\n",
       "        [0, 0, 1, 1],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 0, 1, 0]], dtype=uint16)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
