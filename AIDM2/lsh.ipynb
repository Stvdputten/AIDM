{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Advances in Data Mining**\n",
    "\n",
    "Stephan van der Putten | (s1528459) | stvdputtenjur@gmail.com  \n",
    "Theo Baart | s2370328 | s2370328@student.leidenuniv.nl\n",
    "\n",
    "### **Assignment 2**\n",
    "This assignment is concerned with finding the set of similar users in the provided datasource. To be more explicit, in finding all pairs of users who have a Jaccard similarity of more than 0.5. Additionally, this assignment considers comparing the \"naïve implementation\" with the \"LSH implementation\". The \"naïve implementation\" can be found in the file `time_estimate.ipynb` and the \"LSH implementation\" in the file `lsh.ipynb`.\n",
    "\n",
    "Note all implementations are based on the assignment guidelines and helper files given as well as the documentation of the used functions. \n",
    "\n",
    "\n",
    "POSSIBLE SOURCES:\n",
    "<http://www.hcbravo.org/dscert-mldm/projects/project_1/>\n",
    "<https://colab.research.google.com/drive/1HetBrWFRYqwUxn0v7wIwS7COBaNmusfD#scrollTo=hzPw8EMoW4i4&forceEdit=true&sandboxMode=true>\n",
    "\n",
    "\n",
    "#### **LSH Implementation**\n",
    "This notebook implements LSH in order to find all pairs of users with a Jaccard similarity of more than 0.5. As noted in the assignment instructions the data file is loaded from `user_movie.npy` and the list of user pairs are printed in the file `ans.txt`. Additionally, this implementation supports the setting of a random seed to determine the permutations to be used in LSH. The algorithm will continually save its output so as to aid in the evluation criteria which only looks at the first 15 minutes of the LSH execution.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet handles all imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, csc_matrix, coo_matrix, lil_matrix, find\n",
    "from scipy.sparse import identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Program Execution**\n",
    "This section is concerned with parsing the input arguments and determining the execution flow of the program.\n",
    "\n",
    "___\n",
    "The `main` function handles the start of execution from the command line.\n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `argv` - the command line arguments given to the program\n",
    "  \n",
    "The following command line arguments are expected:\n",
    "  * `seed` - the value to use as random seed\n",
    "  * `path` - the location of the `user_movies.npy` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class LSH():\n",
    "    \n",
    "    def __init__(self, dataset=None, sparse_matrix=None, \n",
    "                 signature_length=50, minhash=None, hash_func=None, sigm=None):\n",
    "        self.dataset = dataset\n",
    "        self.signature_length = signature_length\n",
    "        self.sparse_matrix = sparse_matrix\n",
    "        self.minhash = minhash\n",
    "        self.hash_func = hash_func\n",
    "        self.sigm = sigm\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.dataset = np.load(path)\n",
    "     \n",
    "    def create_matrix(self):\n",
    "        c = dataset[:,0]\n",
    "        r = dataset[:,1]\n",
    "        d = np.ones(len(c))\n",
    "        max_c = len(np.unique(c))\n",
    "        max_r = len(np.unique(r))\n",
    "        self.sparse_matrix = csr_matrix((d,(r,c)), shape=(max_r, max_c))\n",
    "        \n",
    "    def create_hashfunc():\n",
    "        self.hash_func = np.array([np.random.permutation(self.sparse_matrix.shape[0]) for i in range(signature_length)])\n",
    "        \n",
    "    def minhash():\n",
    "        sigm = np.full((self.signature_length, self.sparse_matrix.shape[1]), np.inf)\n",
    "        for row in range(self.sparse_matrix.shape[0]):\n",
    "            ones = find(self.sparse_matrix[row, :])[1]\n",
    "            hash = self.hash_func[:,row]\n",
    "            B = sigm.copy()\n",
    "            B[:,ones] = 1\n",
    "            B[:,ones] = np.multiply(B[:,ones], hash.reshape((len(hash), 1)))\n",
    "            sigm = np.minimum(sigm, B)\n",
    "        self.sigm = sigm\n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "user_movie = np.load('datasets/user_movie.npy')\n",
    "lsh = LSH(dataset=user_movie)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.6 s, sys: 625 ms, total: 5.22 s\n",
      "Wall time: 5.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "c = user_movie[:,0]\n",
    "r = user_movie[:,1]\n",
    "d = np.ones(len(c))\n",
    "max_c = len(np.unique(c))\n",
    "max_r = len(np.unique(r))\n",
    "# m = csr_matrix((d, (r,c)), shape=(max_r, max_c))\n",
    "csc = csc_matrix((d, (r,c)), shape=(max_r, max_c))\n",
    "csr = csr_matrix((d, (r,c)), shape=(max_r, max_c))\n",
    "signature_length = 50\n",
    "\n",
    "# example = np.array([[1,0,0,1],[0,0,1,0],[0,1,0,1],[1,0,1,0],[0,0,1,0]])\n",
    "# hash_func = np.array([[4,3,1,2,0], [3,0,4,2,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def rowminhash(signature_length, hash_func, matrix):\n",
    "    sigm = np.full((signature_length, matrix.shape[1]), np.inf)\n",
    "    for row in range(matrix.shape[0]):\n",
    "        ones = find(matrix[row, :])[1]\n",
    "        hash = hash_func[:,row]\n",
    "        B = sigm.copy()\n",
    "        B[:,ones] = 1\n",
    "        B[:,ones] = np.multiply(B[:,ones], hash.reshape((len(hash), 1)))\n",
    "        sigm = np.minimum(sigm, B)\n",
    "    return(sigm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "import math\n",
    "\n",
    "def choose_nbands(t, n):\n",
    "    def error_fun(x):\n",
    "        cur_t = (1/x[0])**(x[0]/n)\n",
    "        return (t-cur_t)**2\n",
    "\n",
    "    opt_res = opt.minimize(error_fun, x0=(10), method='Nelder-Mead')\n",
    "    b = int(math.ceil(opt_res['x'][0]))\n",
    "    r = int(n / b)\n",
    "    final_t = (1/b)**(1/r)\n",
    "    return b, final_t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def do_lsh(sign_matrix, signature_length, threshold):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 3\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "threshold=0.5\n",
    "numhashes = signature_length\n",
    "b, _ = choose_nbands(threshold, numhashes)\n",
    "r = int(numhashes / b)\n",
    "print(b, r)\n",
    "\n",
    "n_col = len(csc.shape[1])\n",
    "for band in range(b):\n",
    "    # figure out which rows of minhash signature matrix to hash for this band\n",
    "    start_index = int(band * r)\n",
    "    end_index = min(start_index + r, numhashes)\n",
    "\n",
    "    # initialize hashtable for this band\n",
    "    cur_buckets = defaultdict(list)\n",
    "    \n",
    "    for j in range(n_col):\n",
    "      # THIS IS WHAT YOU NEED TO IMPLEMENT\n",
    "# http://www.hcbravo.org/dscert-mldm/projects/project_1/\n",
    "#     https://colab.research.google.com/drive/1HetBrWFRYqwUxn0v7wIwS7COBaNmusfD#scrollTo=hzPw8EMoW4i4&forceEdit=true&sandboxMode=true\n",
    "    # add this hashtable to the list of hashtables\n",
    "    buckets.append(cur_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsh_r_bucket_to_id(u,id,b,r):\n",
    " u=u.T\n",
    " id = np.array(id)\n",
    " number_of_users = u.shape[0]\n",
    " \n",
    " r_bucket_to_id = [dict() for x in range(b)]\n",
    " \n",
    " t1 = time.time()\n",
    " \n",
    " for i in range(number_of_users):\n",
    "   \n",
    "   if i % 10000==0:\n",
    "     print(str(round(100*i/number_of_users,2))+' percent complete in '+str(round(time.time()-t1,2))+ ' seconds')\n",
    "   \n",
    "   row = u[i,:]    \n",
    "   for j in range(b):\n",
    "     r_signature = str(row[j*r:(j+1)*r])\n",
    "     \n",
    "     if r_signature in r_bucket_to_id[j]:\n",
    "       r_bucket_to_id[j][r_signature] = r_bucket_to_id[j][r_signature]+[id[i]]\n",
    "       \n",
    "     else :\n",
    "       r_bucket_to_id[j][r_signature] = [id[i]]\n",
    "\n",
    " return r_bucket_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigm100 = np.load('datasets/sign_matrix_100.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# to = time.time()\n",
    "\n",
    "threshold=0.55\n",
    "numhashes = signature_length\n",
    "numhashes = 100\n",
    "b, _ = choose_nbands(threshold, numhashes)\n",
    "r = int(numhashes / b)\n",
    "# r_bucket_to_id = lsh_r_bucket_to_id(sigm2,docid,b,r)\n",
    "# r_bucket_to_id = lsh_r_bucket_to_id(sigm100,docid,b,r)\n",
    "\n",
    "# print(round(time.time()-to,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41491326668312173"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5492802716530588 20 5\n"
     ]
    }
   ],
   "source": [
    "print(_,b,r)\n",
    "# r_bucket_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 4]), array([0, 1, 2, 1, 1]))\n",
      "[[1 0 0 1 0 1 0 0 1 0]\n",
      " [2 3 1 5 6 2 3 1 5 6]\n",
      " [0 1 4 1 1 0 1 4 1 1]\n",
      " [1 0 1 0 0 1 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "example = np.array([[1,2,0,1],\n",
    "                    [0,3,1,0],\n",
    "                    [0,1,4,1],\n",
    "                    [1,5,1,0],\n",
    "                    [0,6,1,0]])\n",
    "hash_func = np.array([[4,3,1,2,0], [3,0,4,2,1]])\n",
    "print(np.unique(example[:,2],return_inverse=True))\n",
    "print(np.vstack((example,example)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103703,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docid = np.array(list(range(csr.shape[1])))#.reshape((1,csr.shape[1]))\n",
    "docid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function vstack in module numpy:\n",
      "\n",
      "vstack(tup)\n",
      "    Stack arrays in sequence vertically (row wise).\n",
      "    \n",
      "    This is equivalent to concatenation along the first axis after 1-D arrays\n",
      "    of shape `(N,)` have been reshaped to `(1,N)`. Rebuilds arrays divided by\n",
      "    `vsplit`.\n",
      "    \n",
      "    This function makes most sense for arrays with up to 3 dimensions. For\n",
      "    instance, for pixel-data with a height (first axis), width (second axis),\n",
      "    and r/g/b channels (third axis). The functions `concatenate`, `stack` and\n",
      "    `block` provide more general stacking and concatenation operations.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    tup : sequence of ndarrays\n",
      "        The arrays must have the same shape along all but the first axis.\n",
      "        1-D arrays must have the same length.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    stacked : ndarray\n",
      "        The array formed by stacking the given arrays, will be at least 2-D.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    stack : Join a sequence of arrays along a new axis.\n",
      "    hstack : Stack arrays in sequence horizontally (column wise).\n",
      "    dstack : Stack arrays in sequence depth wise (along third dimension).\n",
      "    concatenate : Join a sequence of arrays along an existing axis.\n",
      "    vsplit : Split array into a list of multiple sub-arrays vertically.\n",
      "    block : Assemble arrays from blocks.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> a = np.array([1, 2, 3])\n",
      "    >>> b = np.array([2, 3, 4])\n",
      "    >>> np.vstack((a,b))\n",
      "    array([[1, 2, 3],\n",
      "           [2, 3, 4]])\n",
      "    \n",
      "    >>> a = np.array([[1], [2], [3]])\n",
      "    >>> b = np.array([[2], [3], [4]])\n",
      "    >>> np.vstack((a,b))\n",
      "    array([[1],\n",
      "           [2],\n",
      "           [3],\n",
      "           [2],\n",
      "           [3],\n",
      "           [4]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ?np.vstackb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 13s, sys: 2min 47s, total: 7min 1s\n",
      "Wall time: 7min 1s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed = 42\n",
    "\n",
    "# example = csr\n",
    "# example = np.array([[1,0,0,1],[0,0,1,0],[0,1,0,1],[1,0,1,0],[0,0,1,0]])\n",
    "# %time sigm1 = minhash(signature_length,hash_func, example)\n",
    "# signature_length = 100\n",
    "# hash_func = np.array([np.random.permutation(csr.shape[0]) for i in range(signature_length)])\n",
    "# %time sigm1 = rowminhash(signature_length ,hash_func, csr)\n",
    "\n",
    "signature_length = 50\n",
    "hash_func = np.array([np.random.permutation(csr.shape[0]) for i in range(signature_length)])\n",
    "%time sigm2 = rowminhash(signature_length,hash_func, csr)\n",
    "# print(sigm2)\n",
    "# np.save('datasets/sign_matrix_100', sigm1)\n",
    "# np.save('datasets/sign_matrix', sigm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO write out our results ans.txt \n",
    "#TODO set seed in our perm hash function\n",
    "#TODO Find pairs of similar users from buckets\n",
    "#TODO Elegance aka classes, comments/report/citation\n",
    "#TODO IMPROVE EFFICIENCY FOR LONGER SIGNATURES Time < 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    seed = sys.argv[1]\n",
    "    path = sys.argv[2]\n",
    "    print(seed, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet passes the start of the program and the command line arguments to the `main` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv[1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}