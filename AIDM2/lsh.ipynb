{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Advances in Data Mining**\n",
    "\n",
    "Stephan van der Putten | (s1528459) | stvdputtenjur@gmail.com  \n",
    "Theo Baart | s2370328 | s2370328@student.leidenuniv.nl\n",
    "\n",
    "### **Assignment 2**\n",
    "This assignment is concerned with finding the set of similar users in the provided datasource. To be more explicit, in finding all pairs of users who have a Jaccard similarity of more than 0.5. Additionally, this assignment considers comparing the \"naïve implementation\" with the \"LSH implementation\". The \"naïve implementation\" can be found in the file `time_estimate.ipynb` and the \"LSH implementation\" in the file `lsh.ipynb`.\n",
    "\n",
    "Note all implementations are based on the assignment guidelines and helper files given as well as the documentation of the used functions. Additionally, the following sources have been referenced and used as inspiration:\n",
    "  1. CMSC643: Machine Learning and Data Mining: <http://www.hcbravo.org/dscert-mldm/projects/project_1/>\n",
    "  2. Shared_Minhash_and_LSH_from_binned_date: <https://colab.research.google.com/drive/1HetBrWFRYqwUxn0v7wIwS7COBaNmusfD#scrollTo=hzPw8EMoW4i4&forceEdit=true&sandboxMode=true>\n",
    "\n",
    "\n",
    "#### **LSH Implementation**\n",
    "This notebook implements LSH in order to find all pairs of users with a Jaccard similarity of more than 0.5. As noted in the assignment instructions the data file is loaded from `user_movie.npy` and the list of user pairs are printed in the file `ans.txt`. Additionally, this implementation supports the setting of a random seed to determine the permutations to be used in LSH. The algorithm will continually save its output so as to aid in the evaluation criteria which only looks at the first 15 minutes of the LSH execution.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Helper Functions**\n",
    "This section contains functions which aid and simplify the code for our LSH implementation\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet handles all imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "import scipy.optimize as opt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `compute_num_bands` function computes the optimal number of bands to use given a threshold and signature size. This is a slightly modified version of the `choose_nbands` function given in [1].\n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `t` - the desired threshold value\n",
    "  * `s` - the size of the signature\n",
    "  \n",
    "The following command line arguments are expected:\n",
    "  * `b` - the suggested number of bands to use\n",
    "  * `final_t` - the computed threshold for this number of bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_num_bands(t, s):\n",
    "    def error_fun(x):\n",
    "        cur_t = (1/x[0])**(x[0]/s)\n",
    "        return (t-cur_t)**2\n",
    "\n",
    "    opt_res = opt.minimize(error_fun, x0=(10), method='Nelder-Mead')\n",
    "    b = int(math.ceil(opt_res['x'][0]))\n",
    "    r = round(n / b)\n",
    "    final_t = (1/b)**(1/r)\n",
    "    return b, final_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate_signature_hash_function` function generates a random hash function which returns a large numeric value to be used for hashing signatures into buckets. This is a slightly modified version of the `make_random_hash_fn` function given in [1].\n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `p` - largest value which can be randomly selected [default: 2^31 - 1]\n",
    "  * `k` - the number of buckets to use [default: 12884901885]\n",
    "  \n",
    "The following command line arguments are expected:\n",
    "  * `lambda` - a lambda function representing the random hash function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_signature_hash_function(p=2**31-1, k=12884901885):#4294967295):\n",
    "    a = np.random.randint(1,p-1)\n",
    "    b = np.random.randint(0, p-1)\n",
    "    return lambda x: ((a * x + b) % p) % k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LSHFunctions**\n",
    "This section contains the functions which execute the various steps of the LSH algorithm.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate_hash_function` function generates a random hash function which returns a large numeric value [i.e. the bucket]. This is a slightly modified version of the `make_random_hash_fn` function given in [1].\n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `p` - largest value which can be randomly selected [default: 2^31 - 1]\n",
    "  * `k` - the number of buckets to use [default: 12884901885]\n",
    "  \n",
    "The following command line arguments are expected:\n",
    "  * `lambda` - a lambda function representing the random hash function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_movie = np.load('datasets/user_movie.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "c = user_movie[:,0]\n",
    "r = user_movie[:,1]\n",
    "d = np.ones(len(c))\n",
    "max_c = len(np.unique(c))\n",
    "max_r = len(np.unique(r))\n",
    "# m = csr_matrix((d, (r,c)), shape=(max_r, max_c))\n",
    "csc = csc_matrix((d, (r,c)), shape=(max_r, max_c))\n",
    "csr = csr_matrix((d, (r,c)), shape=(max_r, max_c))\n",
    "signature_length = 50\n",
    "\n",
    "# example = np.array([[1,0,0,1],[0,0,1,0],[0,1,0,1],[1,0,1,0],[0,0,1,0]])\n",
    "# hash_func = np.array([[4,3,1,2,0], [3,0,4,2,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example = np.array([[1,0,0,1,1,0,0,1],[0,0,1,0,0,0,1,0],[0,0,1,0,0,0,1,0],[0,1,0,1,1,0,0,1],[1,0,1,0,1,0,1,0],[0,1,1,1,1,1,0,1]])\n",
    "hash_func = np.array([[5,4,3,1,2,0],[3,1,2,0,5,4],[1,2,0,5,4,3],[2,0,5,4,3,1],[0,5,4,3,1,2],[3,0,4,2,1,5],[0,4,2,1,5,3],[4,2,1,5,3,0],[2,1,5,3,0,4]])\n",
    "c = csr_matrix(example)\n",
    "s = rowminhash(9, hash_func, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 1, 1, 0, 0, 1],\n",
       "        [0, 0, 1, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 1, 0],\n",
       "        [0, 1, 0, 1, 1, 0, 0, 1],\n",
       "        [1, 0, 1, 0, 1, 0, 1, 0],\n",
       "        [0, 1, 1, 1, 1, 1, 0, 1]], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[5, 4, 3, 1, 2, 0],\n",
       "       [3, 1, 2, 0, 5, 4],\n",
       "       [1, 2, 0, 5, 4, 3],\n",
       "       [2, 0, 5, 4, 3, 1],\n",
       "       [0, 5, 4, 3, 1, 2],\n",
       "       [3, 0, 4, 2, 1, 5],\n",
       "       [0, 4, 2, 1, 5, 3],\n",
       "       [4, 2, 1, 5, 3, 0],\n",
       "       [2, 1, 5, 3, 0, 4]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 0., 0., 0., 0., 2., 0.],\n",
       "       [3., 0., 1., 0., 0., 4., 1., 0.],\n",
       "       [1., 3., 0., 1., 1., 3., 0., 1.],\n",
       "       [2., 1., 0., 1., 1., 1., 0., 1.],\n",
       "       [0., 2., 1., 0., 0., 2., 1., 0.],\n",
       "       [1., 2., 0., 2., 1., 5., 0., 2.],\n",
       "       [0., 1., 2., 0., 0., 3., 2., 0.],\n",
       "       [3., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 3., 0., 2., 0., 4., 0., 2.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(c.todense())\n",
    "display(hash_func)\n",
    "display(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rowminhash(signature_length, hash_func, matrix):\n",
    "    sigm = np.full((signature_length, matrix.shape[1]), np.inf)\n",
    "    for row in range(matrix.shape[0]):\n",
    "        ones = find(matrix[row, :])[1]\n",
    "        hash = hash_func[:,row]\n",
    "        B = sigm.copy()\n",
    "        B[:,ones] = 1\n",
    "        B[:,ones] = np.multiply(B[:,ones], hash.reshape((len(hash), 1)))\n",
    "        sigm = np.minimum(sigm, B)\n",
    "    return(sigm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "def lsh_r_bucket_to_id(sigm,id,b,r,numhashes):\n",
    "    id = np.array(id)\n",
    "    number_of_users = sigm.shape[1]\n",
    "    hash_buckets = defaultdict(list)\n",
    "    hf = make_random_hash_fn()\n",
    "    t1 = time.time()    \n",
    "    for i in range(number_of_users):\n",
    "        if i % 10000==0:\n",
    "            print(str(round(100*i/number_of_users,2))+' percent complete in '+str(round(time.time()-t1,2))+ ' seconds')\n",
    "#         row = u[i,:]  \n",
    "        row = sigm[:,i] \n",
    "        for j in range(b):\n",
    "            r_signature = str(row[j*r:(j+1)*r])\n",
    "            r_hash = hash(r_signature)\n",
    "#             print(r_signature)\n",
    "#             print(r_hash)\n",
    "            r_hash = hf(r_hash)\n",
    "            hash_buckets[r_hash].append(id[i])\n",
    "    hash_buckets_set = {k: set(v) for k,v in hash_buckets.items()}\n",
    "    return hash_buckets_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sigm100 = np.load('datasets/sign_matrix_100.npy')\n",
    "sigm50 = np.load('datasets/sign_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 vs 100\n",
      "0.5492802716530588 20 5\n",
      "0.0 percent complete in 0.0 seconds\n",
      "9.64 percent complete in 30.26 seconds\n",
      "19.29 percent complete in 62.77 seconds\n",
      "28.93 percent complete in 102.01 seconds\n",
      "38.57 percent complete in 136.38 seconds\n",
      "48.21 percent complete in 161.7 seconds\n",
      "57.86 percent complete in 186.94 seconds\n",
      "67.5 percent complete in 214.16 seconds\n",
      "77.14 percent complete in 238.86 seconds\n",
      "86.79 percent complete in 265.06 seconds\n",
      "96.43 percent complete in 289.54 seconds\n"
     ]
    }
   ],
   "source": [
    "sigm = sigm100 \n",
    "# sigm = sigm50\n",
    "threshold=0.57 # overshoot so as to get a similarity matrix closer to 0.5\n",
    "numhashes = sigm.shape[0]\n",
    "# b, _ = choose_nbands(threshold, numhashes)\n",
    "b = 6\n",
    "r = 15\n",
    "r = round(numhashes / b)\n",
    "threshold = (1/b)**(1/r)\n",
    "print(b*r,'vs',numhashes)\n",
    "print(threshold,b,r)\n",
    "user_ids = np.array(list(range(sigm.shape[1])))\n",
    "buckets = lsh_r_bucket_to_id(sigm,user_ids,b,r,numhashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3503486605907668772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-3378229029050999201"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_buckets: 1193619\n",
      "no_candidate_buckets 191783\n",
      "max_bucket_size: 0\n",
      "no_pairs: 0\n",
      "pairs: set()\n",
      "Wall time: 714 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import itertools as it\n",
    "pairs = set()\n",
    "max_l = 0\n",
    "print('no_buckets:',len(buckets))\n",
    "short_buckets = {k: v for k, v in buckets.items() if len(v) >= 2}\n",
    "print('no_candidate_buckets',len(short_buckets))\n",
    "i = 0\n",
    "for v in short_buckets.values():\n",
    "    if len(v) > max_l:\n",
    "        max_l = len(v)\n",
    "    pairs.update(set(it.combinations(v,2)))\n",
    "#         i += 1\n",
    "#         if i == 100:\n",
    "#             break\n",
    "print('max_bucket_size:',max_l)\n",
    "print('no_pairs:',len(pairs))\n",
    "print('pairs:',pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb = short_buckets[4874]\n",
    "print(len(sb),sb)\n",
    "sb = list(sb)\n",
    "base = sigm[:,sb[0]]\n",
    "# print(base)\n",
    "for i in range(1,len(sb)):\n",
    "    far = sigm[:,sb[i]]\n",
    "    intersect = np.intersect1d(base,far)\n",
    "    union = np.union1d(base,far)\n",
    "    print(i,len(intersect)/len(union))\n",
    "#     print(sigm[84:91,i])\n",
    "# print(sigm[84:91,905])\n",
    "# print(sigm[84:91,3831])\n",
    "# print(sigm[84:91,5018])\n",
    "# print(sigm[84:91,23663])\n",
    "# print(sigm[84:91,23490:23500].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed = 42\n",
    "\n",
    "# example = csr\n",
    "# example = np.array([[1,0,0,1],[0,0,1,0],[0,1,0,1],[1,0,1,0],[0,0,1,0]])\n",
    "# %time sigm1 = minhash(signature_length,hash_func, example)\n",
    "# signature_length = 100\n",
    "# hash_func = np.array([np.random.permutation(csr.shape[0]) for i in range(signature_length)])\n",
    "# %time sigm1 = rowminhash(signature_length ,hash_func, csr)\n",
    "\n",
    "signature_length = 50\n",
    "hash_func = np.array([np.random.permutation(csr.shape[0]) for i in range(signature_length)])\n",
    "%time sigm2 = rowminhash(signature_length,hash_func, csr)\n",
    "# print(sigm2)\n",
    "# np.save('datasets/sign_matrix_100', sigm1)\n",
    "# np.save('datasets/sign_matrix', sigm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO write out our results ans.txt \n",
    "#TODO set seed in our perm hash function\n",
    "#TODO Find pairs of similar users from buckets\n",
    "#TODO Elegance aka classes, comments/report/citation\n",
    "#TODO IMPROVE EFFICIENCY FOR LONGER SIGNATURES Time < 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Program Execution**\n",
    "This section is concerned with parsing the input arguments and determining the execution flow of the program.\n",
    "\n",
    "___\n",
    "The `main` function handles the start of execution from the command line.\n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `argv` - the command line arguments given to the program\n",
    "  \n",
    "The following command line arguments are expected:\n",
    "  * `seed` - the value to use as random seed\n",
    "  * `path` - the location of the `user_movies.npy` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    seed = sys.argv[1]\n",
    "    path = sys.argv[2]\n",
    "    print(seed, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet passes the start of the program and the command line arguments to the `main` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv[1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
