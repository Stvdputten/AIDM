{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Advances in Data Mining**\n",
    "\n",
    "Stephan van der Putten | (s1528459) | stvdputtenjur@gmail.com  \n",
    "Theo Baart | s2370328 | s2370328@student.leidenuniv.nl\n",
    "\n",
    "### **Assignment 1**\n",
    "This assignment is concered with implementing formulas and models capable of predicting movie ratings for a set of users. Additionally, the accuracy of the various models are checked.\n",
    "\n",
    "Note all implementations are based on the assignment guidelines and helper files given as well as the documentation of the used functions.\n",
    "\n",
    "#### **Naive Approaches**\n",
    "This specific notebook handles the implementation of various naive approaches/formulas to the prediction problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet handles all imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Extraction and Preparation**\n",
    "\n",
    "The `convert_data` function is used to extract the data from the raw data file and store it in a format that is more convenient for us. \n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `path` - the (relative) location of the raw dataset (with filetype `.dat`)\n",
    "  * `cols` - which columns to load from the raw dataset\n",
    "  * `delim` - the delimitor used in the raw dataset\n",
    "  * `dt` - the datatype used for the data in the raw dataset\n",
    "    \n",
    "Additionally it returns the following value:\n",
    "  * `path` - the location at which the converted dataset is stored (with filetype `.npy`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(path=\"datasets/ratings\",cols=(0,1,2),delim=\"::\",dt=\"int\"):\n",
    "    raw = np.genfromtxt(path+'.dat', usecols=cols, delimiter=delim, dtype=dt)\n",
    "    np.save(path+\".npy\",raw)\n",
    "    # check to see if file works\n",
    "    assert np.load(path+'.npy').all() == raw.all()\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `prep_data` function is used to load the stored data and transform it into a usable and well defined dataframe. \n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `path` - the (relative) location of the converted dataset: if no file exists a new one is created\n",
    "    \n",
    "Additionally it returns the following value:\n",
    "  * `df_ratings` - a dataframe containing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(path='datasets/ratings'):\n",
    "    filepath = path+'.npy'\n",
    "    if not os.path.isfile(filepath):\n",
    "        filepath = convert_data()\n",
    "    ratings = np.load(filepath)\n",
    "    df_ratings = pd.DataFrame(ratings)\n",
    "    colnames = ['UserId', 'MovieId', 'Rating']\n",
    "    df_ratings.columns = colnames\n",
    "    return df_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet is responsible for running the extraction and preparation of the raw data. The data is stored in `df_ratings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = prep_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Rating Global**\n",
    "\n",
    "The `rating_global` function predicts the user's ratings for a certain movie by taking the mean of all the ratings in the dataset. \n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `df` - the dataframe containing the dataset\n",
    "  * `user` - the user for which a rating is requested [not used, exists for compatibility with generic functions]\n",
    "  * `item` - the movie for which a rating is requested [not used, exists for compatibility with generic functions]\n",
    "    \n",
    "Additionally it returns the following value:\n",
    "  * `rating` - the predicted rating for the requested movie by the requested user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_global(df,user='',item=''):\n",
    "    rating = df['Rating'].mean()\n",
    "    return rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet executes a test run of the rating function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.581564453029317\n"
     ]
    }
   ],
   "source": [
    "example = rating_global(df=df_ratings,user=1,item=1193)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Rating Item**\n",
    "\n",
    "The `rating_item` function predicts the user's ratings for a certain movie by taking the mean of all the ratings in the dataset for that specific movie.\n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `df` - the dataframe containing the dataset\n",
    "  * `item` - the movie for which a rating is requested\n",
    "  * `user` - the user for which a rating is requested [not used, exists for compatibility with generic functions]\n",
    "\n",
    "Additionally it returns the following value:\n",
    "  * `rating` - the predicted rating for the requested movie by the requested user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_item(df,item,user=''):\n",
    "    rating = df[df['MovieId']== item].groupby('MovieId')['Rating'].mean()\n",
    "    return rating[item]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet executes a test run of the rating function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.390724637681159\n"
     ]
    }
   ],
   "source": [
    "example = rating_item(df=df_ratings,user=1,item=1193)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Rating User**\n",
    "\n",
    "The `rating_user` function predicts the user's ratings for a certain movie by taking the mean of all the ratings in the dataset by the specific user. \n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `df` - the dataframe containing the dataset\n",
    "  * `user` - the user for which a rating is requested\n",
    "  * `item` - the movie for which a rating is requested [not used, exists for compatibility with generic functions]\n",
    "    \n",
    "Additionally it returns the following value:\n",
    "  * `rating` - the predicted rating for the requested movie by the requested user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_user(df,user,item=''):\n",
    "    rating = df[df['UserId']== user].groupby('UserId')['Rating'].mean()\n",
    "    return rating[user]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet executes a test run of the rating function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.188679245283019\n"
     ]
    }
   ],
   "source": [
    "example = rating_user(df=df_ratings,user=1,item=1193)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Rating User-Item**\n",
    "\n",
    "The `rating_user_item` function predicts the user's ratings for a certain movie by applying a linear regression to the outputs of the `rating_user` and `rating_item` functions. \n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `df` - the dataframe containing the dataset\n",
    "  * `user` - the user for which a rating is requested\n",
    "  * `item` - the movie for which a rating is requested  \n",
    "  * `alpha` - the weight for the `rating_user` function\n",
    "  * `beta` - the weight for the `rating_item` function\n",
    "  * `gamma` - the offset/modifier for the linear regression\n",
    "    \n",
    "Additionally it returns the following value:\n",
    "  * `rating` - the predicted rating for the requested movie by the requested user    \n",
    "  \n",
    "Note: `alpha`, `beta` and `gamma` are estimated by the `run_linear_regression` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_user_item(df,user,item,alpha=0.78212853,beta=0.87673970,gamma=-2.35619748):\n",
    "    mean_user = rating_user(df,user)\n",
    "    mean_item = rating_item(df,item)\n",
    "    rating = alpha * mean_user + beta * mean_item + gamma\n",
    "\n",
    "    return rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate_store_matrix` function is responsible for generating the input matrix for the `run_linear_regression` function. This matrix consists of the user rating, movie rating and a constant term. For easier (re)use the matrix stored into a file.\n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `df` - the dataframe containing the dataset\n",
    "  * `path` - the path where the matrix should be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_store_matrix(df,path='datasets/inputLM'):  \n",
    "    r_user = df.groupby('UserId')['Rating'].mean()\n",
    "    r_item = df.groupby('MovieId')['Rating'].mean()\n",
    "    \n",
    "    matrix = []\n",
    "    for index, row in df.iterrows():\n",
    "        matrix.append([index, r_user[row['UserId']], r_item[row['MovieId']], 1])\n",
    "    np.save(path+'.npy', matrix)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `run_linear_regression` function estimates the values needed for `alpha`, `beta` and `gamma`.\n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `df` - the dataframe containing the dataset\n",
    "  * `path` - the location of the input matrix: if no file exists at this location one will be generated\n",
    "\n",
    "Additionally it returns the following values:\n",
    "  * `alpha` - the estimated weight for the `rating_user` function\n",
    "  * `beta` - the estimated weight for the `rating_item` function\n",
    "  * `gamma` - the estimated offset/modifier for the linear regression\n",
    "  \n",
    "Note it is assumed that the indexes in `df` and the matrix stored at `path` correspond, e.g. the data in row 0 of the matrix is computed based on the values of the data in row 0 of `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_linear_regression(df, path='datasets/inputLM'):\n",
    "    filepath = path+'.npy'\n",
    "    if not os.path.isfile(filepath):\n",
    "        generate_store_matrix(df,path)\n",
    "    matrix = np.load(filepath) \n",
    "    y = df['Rating']\n",
    "    S = np.linalg.lstsq(matrix,y,rcond=None)\n",
    "    alpha =S[0][0]\n",
    "    beta = S[0][1]\n",
    "    gamma = S[0][2]\n",
    "    print('Sum Squared Error: '+str(S[1]))\n",
    "    return alpha, beta, gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet executes a test run of the rating function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum Squared Error: [838481.41893203]\n",
      "4.765019952559127\n"
     ]
    }
   ],
   "source": [
    "a,b,g = run_linear_regression(df_ratings)\n",
    "example = rating_user_item(df=df_ratings,user=1,item=1193,alpha=a,beta=b,gamma=g)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cross-validation**\n",
    "\n",
    "The accuracy of each rating function is computed through 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `run_validation` function executes `n`-fold validation for a given function and initial data set. The initial data is split into `n` test and training folds for which the error is computed. The average error gives an indication of the accuracy of the rating function. \n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `df` - the dataframe containing the original dataset\n",
    "  * `function` a string representing name of the function to be tested\n",
    "  * `n` - the number of folds to be generated\n",
    "  * `seed` - the random seed to be used\n",
    "    \n",
    "Additionally it returns the following value:\n",
    "  * `train_error` - the average error for this function on the training set\n",
    "  * `test_error` - the average error for this function on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(df,function,n=5,seed=17092019):\n",
    "    err_train = np.zeros(n)\n",
    "    err_test = np.zeros(n)\n",
    "    \n",
    "    kf = KFold(n_splits=n, shuffle=True,random_state=seed)\n",
    "#     print(kf)\n",
    "\n",
    "    i = 0\n",
    "    for train_index, test_index in kf.split(df):\n",
    "        df_train, df_test = df.iloc[train_index].copy(), df.iloc[test_index].copy()\n",
    "        \n",
    "        # run function on training set\n",
    "        df_train.loc[:,'RatingTrained'] = [function(df=df_train,user=u,item=i) for u, i in zip(df_train['UserId'],df_train['MovieId'])]\n",
    "#         print(i,'trained')\n",
    "        # compute error on train set\n",
    "        df_train.loc[:,'DiffSquared'] = [(t - r)**2 for t, r in zip(df_train['RatingTrained'],df_train['Rating'])]\n",
    "        err_train[i] = np.sqrt(np.mean(df_train['DiffSquared']))\n",
    "#         print(i,'train error')\n",
    "        # compute error on test set\n",
    "        df_test.loc[:,'DiffSquared'] = [(function(df=df_test,user=u,item=i) - r)**2 for u, i, r in zip(df_test['UserId'],df_test['MovieId'],df_test['Rating'])]\n",
    "        err_test[i] = np.sqrt(np.mean(df_test['DiffSquared']))  \n",
    "#         print(i,'test trained and error')\n",
    "        i = i + 1\n",
    "    # compute total error\n",
    "    train_error = np.mean(err_train)\n",
    "    test_error = np.mean(err_test)\n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet computes the mean train and test errors for the `rating_global` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = run_validation(df_ratings,rating_global)\n",
    "print(\"Mean training error: \" + str(train))\n",
    "print(\"Mean test error: \" + str(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet computes the mean train and test errors for the `rating_item` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = run_validation(df_ratings,rating_item)\n",
    "print(\"Mean training error: \" + str(train))\n",
    "print(\"Mean test error: \" + str(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet computes the mean train and test errors for the `rating_user` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = run_validation(df_ratings,rating_user)\n",
    "print(\"Mean training error: \" + str(train))\n",
    "print(\"Mean test error: \" + str(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet computes the mean train and test errors for the `rating_user_item` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = run_validation(df_ratings,rating_user_item)\n",
    "print(\"Mean training error: \" + str(train))\n",
    "print(\"Mean test error: \" + str(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cross-validation Results**\n",
    "\n",
    "As can be seen by running the `run_validation` function for the various rating function the performance for each function vastly differs. This is obvious as each function takes a more detailed look (i.e. considers more factors) into what could influence the rating. For a comparison of these results with the model (matrix factorization with gradient descent) please see the `Assignment1_Group15_Model` notebook. \n",
    "\n",
    "Below is an ordered list ranking the different naive functions from most accurate (lowest mean test error) to least accurate:\n",
    "  1. `rating_user_item` - test error of 0.900 and training error of 0.914\n",
    "  2. `rating_item` - test error of 0.967 and training error of 0.974 \n",
    "  3. `user_user` - test error of 1.016 and training error of 1.027\n",
    "  4. `rating_global` - test error of 1.117 and training error of 1.117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conclusion Naive**\n",
    "\n",
    "In this notebook we calculated 4 different models naively. The results show that combining user/item means to optimise RMSE will give the best results. Although a simple implementation, it does provide a reasonable improvement compared to simply using global, user or item mean. Despite these improvements, it should be noted that generating this model is far from efficent. In this regard there is still much room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippets load and print the cross-validation results. These results are also shown and discussed in section *Cross-validation results*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_r_global = np.load('datasets/train_r_global.npy')\n",
    "test_r_global = np.load('datasets/test_r_global.npy')\n",
    "train_r_item = np.load('datasets/train_r_item.npy')\n",
    "test_r_item = np.load('datasets/test_r_item.npy')\n",
    "train_r_user = np.load('datasets/train_r_user.npy')\n",
    "test_r_user = np.load('datasets/test_r_user.npy')\n",
    "train_r_user_item = np.load('datasets/train_r_user_item.npy')\n",
    "test_r_user_item = np.load('datasets/test_r_user_item.npy')\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean training error global 1.1171012362168529\n",
      "Mean test error global 1.117100475302129\n",
      "\n",
      "Mean training error item :0.9742186635932286\n",
      "Mean test error item 0.967249146086768\n",
      "\n",
      "Mean training error user 1.02767651582948\n",
      "Mean test error user 1.0160458302339375\n",
      "\n",
      "Mean training error user/item :0.9146331503151405\n",
      "Mean test error user/item 0.9005852088723725\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean training error global \" + str(train_r_global))\n",
    "print(\"Mean test error global \" + str(test_r_global))\n",
    "print()\n",
    "print(\"Mean training error item :\" + str(train_r_item))\n",
    "print(\"Mean test error item \" + str(test_r_item))\n",
    "print()\n",
    "print(\"Mean training error user \" + str(train_r_user))\n",
    "print(\"Mean test error user \" + str(test_r_user))\n",
    "print()\n",
    "print(\"Mean training error user_item :\" + str(train_r_user_item))\n",
    "print(\"Mean test error user_item \" + str(test_r_user_item))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
