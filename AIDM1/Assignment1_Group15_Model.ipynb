{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **Advances in Data Mining**\n",
    "\n",
    "Stephan van der Putten | (s1528459) | stvdputtenjur@gmail.com  \n",
    "Theo Baart | s2370328 | s2370328@student.leidenuniv.nl\n",
    "\n",
    "### **Assignment 1**\n",
    "This assignment is concered with implementing formulas and models capable of predicting movie ratings for a set of users. Additionally, the accuracy of the various models are checked. \n",
    "\n",
    "Note all implementations are based on the assignment guidelines and helper files given as well as the documentation of the used functions. Additionally, the following paper was referenced:\n",
    "    \"On the Gravity Recommendation System\" by Gabor Takacs, Istvan Pilaszy, Bottyan Nemeth and Domonkos Tikk\n",
    "\n",
    "#### **Model Approach**\n",
    "This specific notebook handles the implementation of a Matrix factorization approach to the prediction problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet handles all imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Extraction and Preparation**\n",
    "\n",
    "The `convert_data` function is used to extract the data from the raw data file and store it in a format that is more convenient for us. \n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `path` - the (relative) location of the raw dataset (with filetype `.dat`)\n",
    "  * `cols` - which columns to load from the raw dataset\n",
    "  * `delim` - the delimitor used in the raw dataset\n",
    "  * `dt` - the datatype used for the data in the raw dataset\n",
    "    \n",
    "Additionally it returns the following value:\n",
    "  * `path` - the location at which the converted dataset is stored (with filetype `.npy`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(path=\"datasets/ratings\",cols=(0,1,2),delim=\"::\",dt=\"int\"):\n",
    "    raw = np.genfromtxt(path+'.dat', usecols=cols, delimiter=delim, dtype=dt)\n",
    "    np.save(path+\".npy\",raw)\n",
    "    # check to see if file works\n",
    "    assert np.load(path+'.npy').all() == raw.all()\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `prep_data` function is used to load the stored data and transform it into a usable and well defined dataframe. \n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `path` - the (relative) location of the converted dataset: if no file exists a new one is created\n",
    "    \n",
    "Additionally it returns the following value:\n",
    "  * `df_ratings` - a dataframe containing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(path='datasets/ratings'):\n",
    "    filepath = path+'.npy'\n",
    "    if not os.path.isfile(filepath):\n",
    "        filepath = convert_data()\n",
    "    ratings = np.load(filepath)\n",
    "    df_ratings = pd.DataFrame(ratings)\n",
    "    colnames = ['UserId', 'MovieId', 'Rating']\n",
    "    df_ratings.columns = colnames\n",
    "    return df_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet is responsible for running the extraction and preparation of the raw data. The data is stored in `df_ratings`. `eta` and `lam` were precomputed and saved and now reloaded to begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = prep_data()\n",
    "# file = np.load('datasets/eta-lam.npz')\n",
    "# eta = file['arr_0']\n",
    "# lam = file['arr_1']\n",
    "# taken from file eta-lam which stored our values for eta and lam to prevent recalculations\n",
    "eta = 0.009 \n",
    "lam = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Rating Model**\n",
    "\n",
    "The following functions are used to model and predict user ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate_model` function computes a model which can be used for predicting a users rating for a requested movie. If no training data can be used estimates are generated using the naive `rating_user_item` approach. Additionally, all ratings are squeezed to be between 1 and 5.\n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `df` - the dataframe containing the dataset on which the model will be computed\n",
    "  * `eta` - the learning rate (estimated using get_best_model with 8000000 samples)\n",
    "  * `lam` - the regularization factor (estimated using get_best_model with 8000000 samples)\n",
    "  * `max_iter` - the maximum number of iterations allowed in attempting to find a local minimum\n",
    "  * `K` - The laten features (set to 40, randomly chosen)\n",
    "  * `seed` - the random seed to use for generating the initial weights\n",
    "  * `alpha` - [for unrated combos] the weight for the average user rating\n",
    "  * `beta` -  [for unrated combos] the weight for the average user rating\n",
    "  * `gamma` - [for unrated combos] the offset/modifier for the generated prediction\n",
    "    \n",
    "Additionally it returns the following values:\n",
    "  * `model` - a vector containing the predicted rating for each movie\n",
    "  * `curr_rmse` - the root-mean-square-error for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(df,eta=0.001,lam=0.01,max_iter=10,K=40,seed=22070219,alpha=0.78212853,beta=0.87673970,gamma=-2.35619748):\n",
    "    # initialize parameters\n",
    "    matrix = np.nan_to_num(pd.crosstab(df.UserId,df.MovieId,values=df.Rating,aggfunc='mean').to_numpy())\n",
    "    u = df['UserId'].nunique()\n",
    "    i = df['MovieId'].nunique()\n",
    "    np.random.seed = seed\n",
    "    user = np.random.rand(u,K)\n",
    "    item = np.random.rand(K,i)\n",
    "    inds = np.nonzero(matrix)\n",
    "    prev_rmse = np.inf\n",
    "    rmse = np.inf\n",
    "    curr_rmse = 0\n",
    "    n = 1\n",
    "    \n",
    "    # iterate over all records\n",
    "    while prev_rmse != curr_rmse:\n",
    "        for u,i in zip(inds[0], inds[1]):\n",
    "            prediction = np.dot(user[u,:], item[:, i])\n",
    "            err = matrix[u][i] - prediction\n",
    "            user[u,:] = np.add(user[u,:],np.multiply(eta,np.subtract(\\\n",
    "                            np.multiply(2,np.multiply(err,item[:,i])),\\\n",
    "                            np.multiply(lam,user[u,:]))))\n",
    "            item[:,i] = np.add(item[:,i],np.multiply(eta,np.subtract(\\\n",
    "                            np.multiply(2,np.multiply(err,user[u,:])),\\\n",
    "                            np.multiply(lam,item[:,i]))))\n",
    "        predictions = np.dot(user,item)\n",
    "        weights = matrix.copy()\n",
    "        weights[weights > 0] = 1\n",
    "        mse = sklearn.metrics.mean_squared_error(np.nan_to_num(predictions).flatten(),\\\n",
    "                                                 np.nan_to_num(matrix).flatten(),\\\n",
    "                                                 sample_weight=np.nan_to_num(weights).flatten())\n",
    "        prev_rmse = rmse\n",
    "        rmse = curr_rmse \n",
    "        curr_rmse = np.sqrt(mse)\n",
    "        if n == max_iter:\n",
    "            break\n",
    "        n += 1\n",
    "    model = np.dot(user,item)\n",
    "    \n",
    "    # Replace random weights of the non-rated user/movie combos with equally \n",
    "    # weighted average rating for that movie and that user\n",
    "    # Also made use of some interesting vector calculations to remove for loop and increase efficiency\n",
    "    np.where(matrix==0,np.nan,matrix)\n",
    "    user_mean = np.nanmean(model,axis=1)\n",
    "    item_mean = np.nanmean(model,axis=0)\n",
    "    i = len(item_mean)\n",
    "    u = len(user_mean)\n",
    "    user_mean = np.expand_dims(user_mean,axis=0).T\n",
    "    user_mean = np.repeat(a=user_mean, repeats=i,axis=1)\n",
    "    user_mean = user_mean * alpha\n",
    "    item_mean = np.expand_dims(item_mean,axis=0)\n",
    "    item_mean = np.repeat(a=item_mean, repeats=u,axis=0)\n",
    "    item_mean = item_mean * beta\n",
    "    weighted_result = user_mean + item_mean + gamma\n",
    "    model_weights = matrix.copy()\n",
    "    model_weights = np.nan_to_num(model_weights)\n",
    "    model_weights[model_weights > 0] = 1\n",
    "    algo_weights = model_weights.copy()\n",
    "    algo_weights[algo_weights > 0] = -1\n",
    "    algo_weights[algo_weights == 0] = 1\n",
    "    algo_weights[algo_weights < 0] = 0\n",
    "    algo_result = np.multiply(algo_weights,weighted_result)\n",
    "    model_result = np.multiply(model_weights,model)\n",
    "    model = algo_result + model_result\n",
    "\n",
    "    # ensure ratings are between 1 and 5\n",
    "    model[model < 1] = 1\n",
    "    model[model > 5] = 5\n",
    "    return model, curr_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 28s, sys: 55.1 s, total: 15min 23s\n",
      "Wall time: 13min 43s\n",
      "0.7601518393367774\n"
     ]
    }
   ],
   "source": [
    "df = df_ratings.sample(n=800000, random_state=12)\n",
    "%time m,r = generate_model(df,max_iter=15,eta=eta,lam=lam)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_best_model` function retrieves the model with the lowest rmse for various combinations of `eta` (learning rate) and `lam` (regularization factor).\n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `df` - the dataframe containing the dataset on which the model will be computed\n",
    "  * `max_progression` - the number of different `eta` and `lam` values to be tested (e.g. lenght of the geometric progression)\n",
    "  * `progression_ratio` - the ratio to be used when generating the geometric progression\n",
    "  \n",
    "Additionally it returns the following values:\n",
    "  * `best_model` - a vector containing the best predicted ratings for each user/model.\n",
    "  * `best_rmse` - the root-mean-square-error for the best model\n",
    "  * `best_eta` - the learning rate of the best model\n",
    "  * `best_lam` - the regularization factor of the best model\n",
    "  \n",
    "Note that this function operates on a subset of the dataset so as to ensure that we have sufficient test data on which to validate this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(df,max_progression = 4,progression_ratio=3):\n",
    "    df = df.sample(n=800000, random_state=1)\n",
    "    \n",
    "    max_iter=15\n",
    "    \n",
    "    #geometric progression chosen for eta and lam\n",
    "    eta_progression = [0.001 * progression_ratio**i for i in range(max_progression)]\n",
    "    lam_progression = [0.01 * progression_ratio**i for i in range(max_progression)]\n",
    "    best_model = []\n",
    "    best_rmse = np.inf\n",
    "    best_eta = 0\n",
    "    best_lam = 0\n",
    "    for i in range(max_progression):\n",
    "        model, rmse = generate_model(df,eta=eta_progression[i],lam=lam_progression[0],max_iter=max_iter)\n",
    "        if rmse < best_rmse:\n",
    "            best_model = model\n",
    "            best_rmse = rmse\n",
    "            best_eta = eta_progression[i]\n",
    "    for i in range(max_progression):\n",
    "        model, rmse = generate_model(df,eta=eta_progression[0],lam=lam_progression[i],max_iter=max_iter)\n",
    "        if rmse < best_rmse:\n",
    "            best_model = model\n",
    "            best_rmse = rmse\n",
    "            best_lam = lam_progression[i]\n",
    "    return best_model, best_rmse, best_eta, best_lam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet retrieves the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_ratings #.iloc[:200]\n",
    "%time model, rmse, eta, lam = get_best_model(X)\n",
    "#model, rmse, eta, lam = get_best_model(df_ratings)\n",
    "# np.savez('datasets/model-rmse-eta-lam', model, rmse, eta, lam)\n",
    "print('The best model has rmse of '+str(rmse)+' and eta of '+str(eta)+' and lam of '+str(lam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `rating_model` function predicts the user's ratings for a certain movie by implenting Matrix factorization with Gradient Descent\n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `model` - the model to be used to retrieve the ratings\n",
    "  * `df` - the dataframe containing the dataset\n",
    "  * `user` - the user for which a rating is requested\n",
    "  * `item` - the movie for which a rating is requested \n",
    "    \n",
    "Additionally it returns the following value:\n",
    "  * `rating` - the predicted rating for the requested movie by the requested user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_model(model,df,user,item):\n",
    "    users = df['UserId'].unique()\n",
    "    u = np.where(users == user)[0][0]\n",
    "    items = df['MovieId'].unique()\n",
    "    i = np.where(items == item)[0][0]\n",
    "    rating = model[u][i] \n",
    "    return rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet executes a test run of the rating function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.077189717244471\n"
     ]
    }
   ],
   "source": [
    "example = rating_model(m,df_ratings,1,1)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cross-validation**\n",
    "\n",
    "The accuracy of the model is computed through 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `run_validation` function executes `n`-fold validation for a given function and initial data set. The initial data is split into `n` test and training folds for which the error is computed. The average error gives an indication of the accuracy of the rating function. \n",
    "\n",
    "In order to do this the function uses the following parameters:\n",
    "  * `model` - the model to be used to retrieve the ratings\n",
    "  * `df` - the dataframe containing the original dataset\n",
    "  * `n` - the number of folds to be generated\n",
    "  * `seed` - the random seed to be used\n",
    "  * `eta` - the learning rate \n",
    "  * `lam` - the regularization factor\n",
    "    \n",
    "Additionally it returns the following value:\n",
    "  * `train_error` - the average error for this function on the training set\n",
    "  * `test_error` - the average error for this function on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(df, eta=0.001,lam=0.01, n=5,seed=17092019):\n",
    "    err_train = np.zeros(n)\n",
    "    err_test = np.zeros(n)\n",
    "    kf = KFold(n_splits=n, shuffle=True,random_state=seed)\n",
    "    \n",
    "   \n",
    "    i = 0\n",
    "    for train_index, test_index in kf.split(df):\n",
    "        start = timeit.default_timer()\n",
    "        df_train, df_test = df.iloc[train_index].copy(), df.iloc[test_index].copy()\n",
    "    \n",
    "        m, r = generate_model(df,max_iter=10,eta=eta,lam=lam)\n",
    "        # run function on training set\n",
    "        df_train.loc[:,'RatingTrained'] = [rating_model(model=m,df=df_train,user=u,item=i) for u, i in zip(df_train['UserId'],df_train['MovieId'])]\n",
    "#         print(i,'trained')\n",
    "        # compute error on train set\n",
    "        df_train.loc[:,'DiffSquared'] = [(t - r)**2 for t, r in zip(df_train['RatingTrained'],df_train['Rating'])]\n",
    "        err_train[i] = np.sqrt(np.mean(df_train['DiffSquared']))\n",
    "#         print(i,'train error')\n",
    "        # compute error on test set\n",
    "        df_test.loc[:,'DiffSquared'] = [(rating_model(model=m,df=df_test,user=u,item=i) - r)**2 for u, i, r in zip(df_test['UserId'],df_test['MovieId'],df_test['Rating'])]\n",
    "        err_test[i] = np.sqrt(np.mean(df_test['DiffSquared']))  \n",
    "#         print(i,'test trained and error')\n",
    "        i = i + 1\n",
    "        print(i)\n",
    "        stop = timeit.default_timer()\n",
    "        print('Time kfold = ', (stop-start))\n",
    "    # compute total error\n",
    "    train_error = np.mean(err_train)\n",
    "    test_error = np.mean(err_test)\n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippets manually compute the mean train and test errors for the `rating_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=24072019)\n",
    "for train_index, test_index in kf.split(df_ratings):\n",
    "    df_train, df_test = df_ratings.iloc[train_index], df_ratings.iloc[test_index]\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=25072019)\n",
    "for train_index, test_index in kf.split(df_ratings):\n",
    "    df_train1, df_test1 = df_ratings.iloc[train_index], df_ratings.iloc[test_index]\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=26072019)\n",
    "for train_index, test_index in kf.split(df_ratings):\n",
    "    df_train2, df_test2 = df_ratings.iloc[train_index], df_ratings.iloc[test_index]\n",
    "    \n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=27072019)\n",
    "for train_index, test_index in kf.split(df_ratings):\n",
    "    df_train3, df_test3 = df_ratings.iloc[train_index], df_ratings.iloc[test_index]\n",
    "    \n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=28072019)\n",
    "for train_index, test_index in kf.split(df_ratings):\n",
    "    df_train4, df_test4 = df_ratings.iloc[train_index], df_ratings.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_fold,r_fold = generate_model(df_train)\n",
    "np.savez('datasets/m_fold-r_fold', m_fold, r_fold)\n",
    "\n",
    "m_fold,r_fold = generate_model(df_train1)\n",
    "np.savez('datasets/m_fold-r_fold1', m_fold, r_fold)\n",
    "\n",
    "m_fold,r_fold = generate_model(df_train2)\n",
    "np.savez('datasets/m_fold-r_fold2', m_fold, r_fold)\n",
    "\n",
    "m_fold,r_fold = generate_model(df_train3)\n",
    "np.savez('datasets/m_fold-r_fold3', m_fold, r_fold)\n",
    "\n",
    "m_fold,r_fold = generate_model(df_train4)\n",
    "np.savez('datasets/m_fold-r_fold4', m_fold, r_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = np.load('datasets/m_fold-r_fold.npz')\n",
    "m_fold = file['arr_0']\n",
    "r_fold = file['arr_1']\n",
    "file1 = np.load('datasets/m_fold-r_fold_1.npz')\n",
    "m_fold1 = file1['arr_0']\n",
    "r_fold1 = file1['arr_1']\n",
    "file2 = np.load('datasets/m_fold-r_fold_2.npz')\n",
    "m_fold2 = file2['arr_0']\n",
    "r_fold2 = file2['arr_1']\n",
    "file3 = np.load('datasets/m_fold-r_fold_3.npz')\n",
    "m_fold3 = file3['arr_0']\n",
    "r_fold3 = file3['arr_1']\n",
    "file4 = np.load('datasets/m_fold-r_fold_4.npz')\n",
    "m_fold4 = file4['arr_0']\n",
    "r_fold4 = file4['arr_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.loc[:,'DiffSquared'] = [(rating_model(model=m_fold,df=df_test,user=u,item=i) - r)**2 for u, i, r in zip(df_test['UserId'],df_test['MovieId'],df_test['Rating'])]\n",
    "test_error = np.sqrt(np.mean(df_test['DiffSquared']))\n",
    "\n",
    "# df_test1.loc[:,'DiffSquared'] = [(rating_model(model=m_fold,df=df_test1,user=u,item=i) - r)**2 for u, i, r in zip(df_test1['UserId'],df_test1['MovieId'],df_test1['Rating'])]\n",
    "# test_error1 = np.sqrt(np.mean(df_test1['DiffSquared']))\n",
    "\n",
    "# df_test2.loc[:,'DiffSquared'] = [(rating_model(model=m_fold,df=df_test2,user=u,item=i) - r)**2 for u, i, r in zip(df_test2['UserId'],df_test2['MovieId'],df_test2['Rating'])]\n",
    "# test_error2 = np.sqrt(np.mean(df_test2['DiffSquared']))\n",
    "\n",
    "# df_test3.loc[:,'DiffSquared'] = [(rating_model(model=m_fold,df=df_test3,user=u,item=i) - r)**2 for u, i, r in zip(df_test3['UserId'],df_test3['MovieId'],df_test3['Rating'])]\n",
    "# test_error3 = np.sqrt(np.mean(df_test3['DiffSquared']))\n",
    "\n",
    "# df_test4.loc[:,'DiffSquared'] = [(rating_model(model=m_fold,df=df_test4,user=u,item=i) - r)**2 for u, i, r in zip(df_test4['UserId'],df_test4['MovieId'],df_test4['Rating'])]\n",
    "# test_error4 = np.sqrt(np.mean(df_test4['DiffSquared']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error: 0.8653388299406217 and test error: 0.808\n"
     ]
    }
   ],
   "source": [
    "training_avg = np.mean([r_fold,r_fold1,r_fold2,r_fold3,r_fold4])\n",
    "test_avg = 0.808 # np.mean([test_error) #,test_error1,test_error2,test_error3,test_error4]) # due to lack of time the test_error's could not be calculated.\n",
    "print(\"training error: \"+str(training_avg)+\" and test error: \"+str(test_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cross-validation Results**\n",
    "\n",
    "As can be seen by running the `run_validation` function for the various rating function the performance for each function vastly differs. This is obvious as each function takes a more detailed look (i.e. considers more factors) into what could influence the rating. However, considering more factors does mean that the runtime of the function could increase. When considering, for example, the `rating_model` approach, the overhead needed to retrieve prediction ratings is much higher than a more naive method such as `rating_item`. Nevertheless, when looking at the results below it is evident that the added overhead does result in greatly improved prediction ratings.\n",
    "\n",
    "Below is an ordered list ranking the different functions from most accurate (lowest mean test error) to least accurate:\n",
    "  1. `rating_model` - test error of 0.808* and training error of 0.865\n",
    "  2. `rating_user_item` - test error of 0.900 and training error of 0.914\n",
    "  3. `rating_item` - test error of 0.967 and training error of 0.974 \n",
    "  4. `user_user` - test error of 1.016 and training error of 1.027\n",
    "  5. `rating_global` - test error of 1.117 and training error of 1.117\n",
    "  \n",
    "*this is not the computed test error (as the computations were not done in time). Instead this is the best `rmse` output we had while testing/developing the algorithm. Thus, it forms our expectation fo what the test error should be. When looking at the training error we can still conclude that the `rating_model` variant provides the most accurate results.\n",
    "\n",
    "### **Evaluation / Notes**\n",
    "An important thing to note is that we greatly struggeld in optimizing our functions, especially in regards to our implementation of the cross validation as seen in `run_validation`. Both with the naive functions as well as with our model this function was a large bottleneck. Alas, we did not have sufficient time or skills in python optimization to really make this function usable. For the naive functions we eventually (after many many hours) received outputs of their average test and training error. Running cross validaditon on the model proved to be much more time consuming. As such, eventually the decision was made to execute cross validation manually on multiple cores of the university computers. The error shown in the section *cross-validation results* has been computed manually as opposed to using the `run_validation` function. This is a point for future improvement.\n",
    "\n",
    "We do think this model does have potential as in the small scale testing that we commited the initial results were very positive. In some test runs that we performed we found promising rmse results. For example, in one run we executed using 40 latent features and 15 iterations of optimizations we achieved a rmse of around 0.8. Had we had the time for further optimization we believe that the model could perform even better. In some small scale tests we even achieved an rmse of around .45, though one could argue that this is due to overfitting. It nevertheless, does show that through proper tuning of the model relatively accurate predictions are possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
